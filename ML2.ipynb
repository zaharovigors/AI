{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Algorithms.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOurKC2mdLsw",
        "colab_type": "text"
      },
      "source": [
        "#### Захаров Игорь Сергеевич\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQIRRuwmdLs0",
        "colab_type": "text"
      },
      "source": [
        "*М8О-308Б-17, №4 по списку*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEoHZ_u_dLs2",
        "colab_type": "text"
      },
      "source": [
        "<center><h2> Алгоритмы </h2></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6vAFEQJdLs3",
        "colab_type": "text"
      },
      "source": [
        "### Общие функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwFD72L8dLs4",
        "colab_type": "text"
      },
      "source": [
        "#### Импорт необходимых библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmUyTYnqdLs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # for grafic\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7H_rPi3dLtB",
        "colab_type": "text"
      },
      "source": [
        "#### Метрики качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHWHBbW9dLtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Accuracy(Y_val, Y_pred):\n",
        "    TP = (Y_val * Y_pred).sum()\n",
        "    TN = np.logical_not(Y_val | Y_pred).sum()\n",
        "    return (TP + TN) / len(Y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u45Afk7zdLtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Precision(Y_val, Y_pred):\n",
        "    TP = (Y_val * Y_pred).sum()\n",
        "    return TP / Y_pred.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymqAdFXWdLtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Recall(Y_val, Y_pred):\n",
        "    TP = (Y_val * Y_pred).sum()\n",
        "    return TP / Y_val.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_2MusEwdLtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def F_metric(Y_val, Y_pred):\n",
        "    precision = Precision(Y_val, Y_pred)\n",
        "    recall = Recall(Y_val, Y_pred)\n",
        "    return 2.0 * recall * precision / (precision + recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmSZmJTWdLtU",
        "colab_type": "text"
      },
      "source": [
        "### Логистическая регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AHk6WAMdLtV",
        "colab_type": "text"
      },
      "source": [
        "#### Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2x68GxadLtV",
        "colab_type": "text"
      },
      "source": [
        "*Логистическая регрессия* - метод, при котором ищутся веса $w$ путём минимизации логистической функции потерь: $w = \\underset{w}{\\operatorname{argmin}}(\\sum\\limits_{i = 1}^{n} \\log{(1 + e^{-y_i \\langle w, x_i \\rangle})} ).$\n",
        "\n",
        "Тогда классифицирующий алгоритм определяется: $a(x) = sign(\\langle w, x \\rangle)$, где вектор $x$ содержит в начале константный еденичный элемент, а множество ответов классификатора: $Y = \\{+1, -1\\}.$\n",
        "\n",
        "При этом метод корректно оценивает вероятности отнесения к положительному классу при помощи сигмоид-функции: $p(y = +1|x) = \\displaystyle\\frac{1}{1 + e^{-\\langle w, x \\rangle}}.$\n",
        "\n",
        "Для обучения модели можно использовать алгоритм *Градиентного спуска* в силу дифференцируемости функционала ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ASwgy5dLtW",
        "colab_type": "text"
      },
      "source": [
        "#### Градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qhnH_axdLtX",
        "colab_type": "text"
      },
      "source": [
        "Регулязационные нормы для градиентного спуска."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNoNiwddLtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2_norm(vector):\n",
        "    return (vector**2).sum()\n",
        "\n",
        "def L1_norm(vector):\n",
        "    return np.abs(vec).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z5DOVtsdLtc",
        "colab_type": "text"
      },
      "source": [
        "Градиенты от норм:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8OZwX8dLtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2_grad(vector):\n",
        "    return vector\n",
        "\n",
        "def L1_grad(vector):\n",
        "    return vector / np.abs(vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY5_nRACdLtf",
        "colab_type": "text"
      },
      "source": [
        "Решатель на основе градиентного спуска:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt8r2qTwdLtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, speed, gradient_func, regulasator=None, \n",
        "                 C=10.0, eps = 0.001, maxsteps=250):\n",
        "        self.speed = speed\n",
        "        self.function = gradient_func\n",
        "        self.maxsteps = maxsteps\n",
        "        self.eps = eps\n",
        "        if regulasator == \"l1\":\n",
        "            self.regulasator = lambda w:  L1_grad(w) / C\n",
        "        elif regulasator == \"l2\":\n",
        "            self.regulasator = lambda w: L2_grad(w) / C\n",
        "        else:\n",
        "            self.regulasator = lambda w: 0.0\n",
        "    \n",
        "    def fit(self, X_train, Y_train):\n",
        "        # init w0\n",
        "        w0 = np.zeros(X_train.shape[1])\n",
        "        w = np.random.random(X_train.shape[1])\n",
        "        k = 1\n",
        "        while np.linalg.norm(w - w0) > self.eps and k <= self.maxsteps:\n",
        "            w0 = w\n",
        "            temp = self.speed * ((1 / k)**0.5) # like vowpal step temp\n",
        "            w = w - temp*(self.function(X_train, Y_train, w) + self.regulasator(w))\n",
        "            k += 1\n",
        "            \n",
        "        return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvQdZExLdLtj",
        "colab_type": "text"
      },
      "source": [
        "#### Логистическая регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLwBvLARdLtj",
        "colab_type": "text"
      },
      "source": [
        "Набор функций, которые непосредственно относятся к логистической регрессии: сигмоида, функция потерь и её градиент по вектору весов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWNS71yLdLtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this exponent numericall stable\n",
        "def sigmoid(x):\n",
        "    return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def logit_loss(wx, y_real):\n",
        "    return np.log(1.0 + np.exp(-wx*y_real)).sum()\n",
        "\n",
        "def logit_grad(x, y, w):\n",
        "    koeff = (y * sigmoid(-y*x.dot(w)))\n",
        "    koeff = koeff.reshape((koeff.shape[0], 1)) # make a column\n",
        "    return -(koeff * x).sum(axis = 0) # full gradient - sum of gradients on ever x[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1QoVtJLdLtn",
        "colab_type": "text"
      },
      "source": [
        "Модель регрессии с использованием градиентного решателя:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7E-C7PUdLto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logistic regression with 2 classes: 0 and 1.\n",
        "class BinaryLogisticRegression:\n",
        "    # main params\n",
        "    def __init__(self, speed = 1.5, reg_type=None, C=2.0, eps=0.001, maxsteps=200):\n",
        "        # init solver\n",
        "        self.solver = GradientDescent(speed, logit_grad, reg_type, C, eps, maxsteps)\n",
        "        # init weight  variable\n",
        "        self.w = None\n",
        "        \n",
        "    # training\n",
        "    def fit(self, X_train, Y_train):\n",
        "        # convert 0 to -1 for algo\n",
        "        Y = np.array(Y_train)\n",
        "        Y[Y_train == 0] = -1\n",
        "        # add np.ones colomn for w0 weight:\n",
        "        x0 = np.ones((X_train.shape[0], 1))\n",
        "        X = np.hstack((x0, X_train))\n",
        "        # train weight by gradient descent\n",
        "        self.w = self.solver.fit(X, Y)\n",
        "        return self\n",
        "    \n",
        "    # returns predictes classes\n",
        "    def predict(self, X_val, border = 0):\n",
        "        # add np.ones colomn for w0 weight:\n",
        "        x0 = np.ones((X_val.shape[0], 1))\n",
        "        X = np.hstack((x0, X_val))\n",
        "        # <w, x> product for all examples\n",
        "        Xw = X.dot(self.w)\n",
        "        # make predict: 0 - negative, 1 - positive\n",
        "        Y_pred = np.zeros(Xw.shape).astype(np.int8)\n",
        "        # a(x) = [<w,x> > t], t - border\n",
        "        Y_pred[Xw >= border] = 1\n",
        "        return Y_pred\n",
        "    \n",
        "    # probs of positive class\n",
        "    def predict_proba(self, X_val):\n",
        "        # add np.ones colomn for w0 weight:\n",
        "        x0 = np.ones((X_val.shape[0], 1))\n",
        "        X = np.hstack((x0, X_val))\n",
        "        # <w, x> product for all examples\n",
        "        Xw = X.dot(self.w)\n",
        "        # return proba\n",
        "        return sigmoid(Xw)\n",
        "    \n",
        "    # compute metrics\n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "    \n",
        "    def weights(self):\n",
        "        return self.w\n",
        "\n",
        "    # for fun\n",
        "    def __str__(self):\n",
        "        return \"Logistic Regression model with gradient descent!\"\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"Logistic Regression\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiB41Q0cdLtr",
        "colab_type": "text"
      },
      "source": [
        "### Метод опорных векторов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhITVjkDdLtr",
        "colab_type": "text"
      },
      "source": [
        "#### Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7H65xWsdLts",
        "colab_type": "text"
      },
      "source": [
        "Метод опорных векторов ищет вектор множетелей Лагранжа: $\\lambda = \\{\\lambda_1, ..., \\lambda_n \\}: \\; \\lambda = \\underset{\\lambda}{\\operatorname{argmax}}(\\sum\\limits_{i = 1}^{n} \\lambda_i - \\displaystyle\\frac{1}{2}\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} \\lambda_i \\lambda_j y_i y_j K(x_i, x_j)).$  Порог $w_o$ расчитывется в процессе итераций.\n",
        "\n",
        "\n",
        "В линейном случае $K(x_i, x_j) = \\langle x_i, x_j \\rangle.$ При этом веса $(w, w_0)$ модели можно найти следующим образом: $w = \\sum\\limits_{i=1}^{n}\\lambda_i y_i x_i$, $w_0 = \\underset{i: \\; 0 < \\lambda_i < C}{\\operatorname{median}}(\\langle w, x_i \\rangle - y_i).$\n",
        "\n",
        "Эта задача решается эффективно при помощи алгоритма **S**equental **M**inimal **O**ptimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHuwxlNSdLts",
        "colab_type": "text"
      },
      "source": [
        "#### Sequental Minimal Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOZMZsIdLtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Binary_SMO:\n",
        "    def __init__(self, X, Y, kernel, C, eps, maxsteps, linear):\n",
        "        self.K = kernel\n",
        "        self.C = C\n",
        "        self.tol = eps\n",
        "        self.maxsteps = maxsteps\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.linear = linear\n",
        "        self.l = np.zeros(Y.shape)\n",
        "        self.e_cache = -Y.astype(np.float64) # zero prediction - y\n",
        "        #self.n = len(X)\n",
        "        self.w0 = 0.0\n",
        "        if linear:\n",
        "            self.w = np.zeros(X.shape[1])\n",
        "\n",
        "\n",
        "    def KKT_violated(self, idx):\n",
        "        r =  self.Y[idx] * self.e_cache[idx]\n",
        "        l = self.l[idx]\n",
        "        # if (M < 1 - tol & l < C) || (M > 1 + tol & l > 0) => KKT violated\n",
        "        # tol - accuracy of computing\n",
        "        return (l < self.C and r < -self.tol) or (l > 0.0 and r > self.tol)\n",
        "\n",
        "    # search by all element ones\n",
        "    def first_heuristic_one(self):\n",
        "        for idx in range(self.l.shape[0]):\n",
        "            if self.KKT_violated(idx):\n",
        "                yield idx\n",
        "\n",
        "    # search by all non-bound elements \n",
        "    def first_heuristic_two(self):\n",
        "        # i think here loop faster then pre-choice by numpy\n",
        "        # because this loop more effective\n",
        "        for idx in range(self.l.shape[0]):\n",
        "            if 0.0 < self.l[idx] < self.C:\n",
        "                if self.KKT_violated(idx):\n",
        "                    yield idx\n",
        "\n",
        "    # search elem that maximaze |E1 - E2| from non-boundary\n",
        "    def second_heuristic_one(self, idx):\n",
        "        # search |E1 - E2| of each elem\n",
        "        dE = np.abs(self.e_cache - self.e_cache[idx])\n",
        "        # all boundary elems not interesting\n",
        "        dE[(self.l >= self.C) | (self.l <= 0.0)] = 0.0\n",
        "        # return i2 = argmax|E1 - E2|\n",
        "        return np.argmax(dE)\n",
        "\n",
        "    def second_heuristic_two(self, idx):\n",
        "        mask = (self.l < self.C) & (self.l > 0.0)\n",
        "        mask[idx] = False\n",
        "        idxes = np.nonzero(mask)[0]\n",
        "        order = np.random.permutation(len(idxes))\n",
        "        return idxes[order]\n",
        "\n",
        "\n",
        "    def second_heuristic_three(self, idx):\n",
        "        #if second heuristic without result, get idxs without elems from second\n",
        "        # it will faster\n",
        "        mask = (self.l >= self.C) | (self.l <= 0.0)\n",
        "        mask[idx] = False\n",
        "        idxes = np.nonzero(mask)[0]\n",
        "        order = np.random.permutation(len(idxes))\n",
        "        return idxes[order]\n",
        "\n",
        "    def get_weights(self):\n",
        "        if self.linear:\n",
        "            return self.w\n",
        "\n",
        "    def get_support(self):\n",
        "        # returns only support vctors with params if you \n",
        "        # dont get them after train\n",
        "        mask = self.l > 0.0\n",
        "        return self.X[mask], self.Y[mask], self.l[mask], self.w0\n",
        "\n",
        "    # return lambdas\n",
        "    def get_coeffs(self):\n",
        "        return self.l\n",
        "\n",
        "    \n",
        "    def optimize_two(self, i1, i2):\n",
        "        # it emulates machine e in computations\n",
        "        eps = 0.00000000000001\n",
        "\n",
        "        y1 = self.Y[i1]\n",
        "        y2 = self.Y[i2]\n",
        "        x1 = self.X[i1]\n",
        "        x2 = self.X[i2]\n",
        "        l1 = self.l[i1]\n",
        "        l2 = self.l[i2]\n",
        "        E1 = self.e_cache[i1]\n",
        "        E2 = self.e_cache[i2]\n",
        "\n",
        "        # compute L H\n",
        "        if y1 == y2:\n",
        "            L = max(0.0, l2 + l1 - self.C)\n",
        "            H = min(self.C, l2 + l1)\n",
        "        else:\n",
        "            L = max(0.0, l2 - l1)\n",
        "            H = min(self.C, self.C + l2 - l1)\n",
        "        if L == H:\n",
        "            return False\n",
        "\n",
        "        # eta\n",
        "        nu = self.K(x1, x1) + self.K(x2, x2) - 2.0*self.K(x1, x2)\n",
        "\n",
        "        # compute l2\n",
        "        if nu > 0.0:\n",
        "            l2 += y2 * (E1 - E2) / nu\n",
        "            if l2 < L:\n",
        "                l2 = L\n",
        "            elif l2 > H:\n",
        "                l2 = H\n",
        "        else:\n",
        "            c1 = nu/2\n",
        "            c2 = y2 * (E1 - E2) + nu * l2\n",
        "            Lobj = c2*L - c1*L*L\n",
        "            Hobj = c2*H - c1*H*H\n",
        "            if Lobj > Hobj + eps:\n",
        "                l2 = L\n",
        "            elif Lobj < Hobj - eps:\n",
        "                l2 = H\n",
        "\n",
        "        if np.abs(l2 - self.l[i2]) < eps*(l2 + self.l[i2] + eps):\n",
        "            return False\n",
        "\n",
        "        # compute l1\n",
        "        l1 -= y1*y2*(l2 - self.l[i2])\n",
        "        \n",
        "        if l1 < 0.0:\n",
        "            l2 += y1*y2*l1\n",
        "            l1 = 0.0\n",
        "        elif l1 > self.C:\n",
        "            l2 += y1*y2*(l1 - self.C)\n",
        "            l1 = self.C\n",
        "\n",
        "        # update w0:\n",
        "        b1 = self.w0 + E1 + y1*(l1 - self.l[i1])*self.K(x1, x1) + y2*(l2 - self.l[i2])*self.K(x1, x2)\n",
        "        b2 = self.w0 + E2 + y1*(l1 - self.l[i1])*self.K(x1, x2) + y2*(l2 - self.l[i2])*self.K(x2, x2)\n",
        "        if l1 > 0.0 and l1 < self.C:\n",
        "            bnew = b1\n",
        "        elif l2 > 0.0 and l2 < self.C:\n",
        "            bnew = b2\n",
        "        else:\n",
        "            bnew = (b1 + b2) / 2.0\n",
        "        \n",
        "        dw0 = bnew - self.w0\n",
        "        self.w0 = bnew\n",
        "\n",
        "        # update E_cache\n",
        "        t1 = y1*(l1 - self.l[i1])\n",
        "        t2 = y2*(l2 - self.l[i2])\n",
        "        \n",
        "        self.e_cache += t1*self.K(self.X, x1) + t2*self.K(self.X, x2) - dw0\n",
        "        \n",
        "        #update w:\n",
        "        if self.linear:\n",
        "            self.w += t1*x1 + t2*x2\n",
        "        # update lambdas:\n",
        "        self.l[i1] = l1\n",
        "        self.l[i2] = l2\n",
        "\n",
        "        return True\n",
        "            \n",
        "            \n",
        "\n",
        "    # search 2 lambdas for optimize and change them\n",
        "    def train(self):\n",
        "        steps = 0\n",
        "        non_bound_loop = True\n",
        "        # main loop\n",
        "        while steps < self.maxsteps:\n",
        "            non_bound_loop ^= True\n",
        "            # outer loops searches i1:\n",
        "            if not non_bound_loop:\n",
        "                # h1-1\n",
        "                changed = False\n",
        "                for idx1 in self.first_heuristic_one():\n",
        "                    # h2-1\n",
        "                    idx2 = self.second_heuristic_one(idx1)\n",
        "                    if self.optimize_two(idx1, idx2):\n",
        "                        changed = True\n",
        "                        continue\n",
        "                    # h2-2,3 together if h2-1 not worked\n",
        "                    # concat idxs of heuristics in sequence\n",
        "                    extra_heuristics = np.concatenate([\n",
        "                        self.second_heuristic_two(idx1),\n",
        "                        self.second_heuristic_three(idx1)\n",
        "                    ])\n",
        "\n",
        "                    for idx2 in extra_heuristics:\n",
        "                        if self.optimize_two(idx1, idx2):\n",
        "                            changed = True\n",
        "                            break\n",
        "                steps += 1\n",
        "                # if nothing changed - work done\n",
        "                if not changed:\n",
        "                    break\n",
        "            else:\n",
        "                # h1-2\n",
        "                # while changing itterate non-bound elements\n",
        "                while changed and steps < self.maxsteps:\n",
        "                    changed = False\n",
        "                    # itterate non-bound elements\n",
        "                    for idx1 in self.first_heuristic_two():\n",
        "                        # h2-1\n",
        "                        idx2 = self.second_heuristic_one(idx1)\n",
        "                        if self.optimize_two(idx1, idx2):\n",
        "                            changed = True\n",
        "                            continue\n",
        "                        # h2-2,3 together if h2-1 not worked\n",
        "                        # concat idxs of heuristics in sequence\n",
        "                        extra_heuristics = np.concatenate([\n",
        "                            self.second_heuristic_two(idx1),\n",
        "                            self.second_heuristic_three(idx1)\n",
        "                        ])\n",
        "                        for idx2 in extra_heuristics:\n",
        "                            if self.optimize_two(idx1, idx2):\n",
        "                                changed = True\n",
        "                                break\n",
        "                    steps += 1\n",
        "        # after train return support vectors with lambdas and Y\n",
        "        mask = self.l > 0.0\n",
        "        return self.X[mask], self.Y[mask], self.l[mask], self.w0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZluEq7p4dLtx",
        "colab_type": "text"
      },
      "source": [
        "#### Метод опорных векторов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoxDB1jPdLtx",
        "colab_type": "text"
      },
      "source": [
        "Примеры ядер для метода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0E01U8BdLty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_kernel(x1, x2):\n",
        "    return x1.dot(x2.T)\n",
        "\n",
        "def rbf_kernel(x1, x2, gamma):\n",
        "    if len(x1.shape) == 1:\n",
        "        x1r = x1.reshape((1, x1.shape[0]))\n",
        "    else:\n",
        "        x1r = x1\n",
        "    if len(x2.shape) == 1:\n",
        "        x2r = x2.reshape((1, x2.shape[0]))\n",
        "    else:\n",
        "        x2r = x2\n",
        "    ans = np.zeros((x1r.shape[0], x2r.shape[0]))\n",
        "    for i in np.arange(x1r.shape[0]):\n",
        "        # \n",
        "        ans[i] = np.exp(-gamma  * ((x2r - x1r[i])**2).T.sum(axis = 0))\n",
        "    if len(x1.shape) == 1:\n",
        "        ans = ans[0]\n",
        "    if len(x2.shape) == 1:\n",
        "        ans = ans.T[0]\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KohCUCtVdLt1",
        "colab_type": "text"
      },
      "source": [
        "Реализация метода опорных векторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNWek1p_dLt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stay kernel None for faster linaear version\n",
        "class BinarySVM:\n",
        "    def __init__(self, kernel = None, C=1.0, eps = 0.001, maxsteps=1000):\n",
        "        self.C = C\n",
        "        self.linear = False\n",
        "        if kernel is None:\n",
        "            kernel = lambda x1, x2: x1.dot(x2.T)\n",
        "            self.linear = True\n",
        "        self.K = kernel\n",
        "        self.w = None\n",
        "        self.w0 = None\n",
        "        self.l = None\n",
        "        self.svX = None\n",
        "        self.svY = None\n",
        "        self.maxsteps = maxsteps\n",
        "        self.eps = eps\n",
        "    \n",
        "    def weights(self):\n",
        "        if self.linear:\n",
        "            return np.append(self.w0, self.w)\n",
        "        \n",
        "    def predict(self, X_val, border = 0):\n",
        "        # make predict: 0 - negative, 1 - positive\n",
        "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
        "        if self.linear:\n",
        "            x0 = np.ones((X_val.shape[0], 1))\n",
        "            X = np.hstack((x0, X_val))\n",
        "            # <w, x> product for all examples\n",
        "            Xw = X.dot(np.append(-self.w0, self.w))\n",
        "            # a(x) = [<w,x> > t], t - border\n",
        "            Y_pred[Xw >= border] = 1\n",
        "            return Y_pred\n",
        "        else:\n",
        "            yl = (self.svY * self.l).reshape((self.svY.shape[0], 1))\n",
        "            U = self.K(yl * self.svX, X_val).sum(axis = 0) - self.w0\n",
        "            Y_pred[U >= border] = 1\n",
        "            return Y_pred\n",
        "\n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "\n",
        "    def fit(self, X_train, Y_train):\n",
        "        X = X_train\n",
        "        Y = np.array(Y_train)\n",
        "        Y[Y_train == 0] = -1\n",
        "        smo = Binary_SMO(X, Y, self.K, self.C, self.eps, self.maxsteps, self.linear)\n",
        "        self.svX, self.svY, self.l, self.w0 = smo.train()\n",
        "        if self.linear:\n",
        "            self.w = smo.get_weights()\n",
        "        return self\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Support Vector Machine\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"SVM\"\n",
        "    \n",
        "    \n",
        "    # returns support vectors with lambdas\n",
        "    def vectors(self):\n",
        "        return self.svX, self.l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BIVUCJYdLt4",
        "colab_type": "text"
      },
      "source": [
        "### Решающее дерево"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSKmeXkXdLt5",
        "colab_type": "text"
      },
      "source": [
        "#### Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2FVxzfTdLt5",
        "colab_type": "text"
      },
      "source": [
        "Все решающие деревья - бинарные деревья, каждой вершине $v$ которых соответствует подвыборки $R_v \\subset (X, Y)$. Для бинарной классификации примем $Y = \\{0, 1 \\}$\n",
        "\n",
        "Вершины делятся на:\n",
        "* Внутренние, содержащие предикаты типа $[f_j (x) < t]$, результат которых делит $R_v$ на две подвыборки $R_l$ и $R_r$, которые будут относиться к левой и правой дочерней вершине соответстенно.\n",
        "* Листы, в которых записан прогноз этого листа $c_v = \\underset{c}{\\operatorname{argmax}} \\sum\\limits^{|R_v|}_{i = 1} [y_i = c]$, где $(x_i, y_i) \\in R_v, \\; c \\in Y$.\n",
        "\n",
        "Деревья зачастую получаются переобученными, поэтому для повышения обобщающей способности пользуются алгоритом срижки деревьев *Cost Complexity Pruning*, который ищет $\\alpha$-оптимальное поддерево - то, у которого значение функционала $R_{\\alpha}(T) = R(T) + \\alpha |T|$ минимально. Мы будем задавать параметр $\\alpha$ в качестве параметра класса, а его оптимальный выбор оставим на совести программиста(можно подобрать на кросс-валидации). Заметим, справедливо: $R_{\\alpha}(T) = R_{\\alpha}(T_l) + R_{\\alpha}(T_r) = \\sum\\limits_{i = 1}^{|T|} R_{\\alpha}(t_{leaf})$, что позволит провести стрижку за один рекурсивный проход снизу вверх по дереву, поскольку минимизаци функционала $\\Delta R_{\\alpha}(T_{sub})$ для любого из поддеревьев $T_{sub}$ будет равна минимизации $\\Delta R_{\\alpha}(T)$ и для самого дерева $T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo72L8phdLt6",
        "colab_type": "text"
      },
      "source": [
        "#### Лист дерева"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE84un6FdLt6",
        "colab_type": "text"
      },
      "source": [
        "Будем хранить во всех вершинах их прогноз и индексы элемнтов из общей выборки вне зависимости от того, листовая она или внутренняя, что упростит ускорит и упростит код, однако ведет к увеличению потребляемой памяти. В внутренних вершинах будет хранится функция - предикат, когда как у листа будет значение $None$.\n",
        "\n",
        "Этот класс нужен для хранения вершин, однако лишен автономности, поскольку управление вершинами не должно происходить в отрыве от дерева. Поэтому функции построения дочерних вершин, подбора параметров и стрижки будут произведены внутри дерева, которое будет иметь полный контроль над классом вершины."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Be67rQRdLt7",
        "colab_type": "text"
      },
      "source": [
        "Это реализация листа с хранением количества объектов разных классов для ускоренной реализации дерева:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l10XhpUZdLt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryNode:\n",
        "    def __init__(self, idxs=None, pos=None, neg=None, c=None):\n",
        "        self.predicat = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.positives = pos\n",
        "        self.negatives = neg\n",
        "        self.c = c\n",
        "        self.idxs = idxs\n",
        "\n",
        "    # setters:\n",
        "    def set_left(self, left_node):\n",
        "        self.left = left_node\n",
        "\n",
        "    def set_idxs(self, idxs):\n",
        "        self.idxs = idxs\n",
        "\n",
        "    def set_right(self, right_node):\n",
        "        self.right = right_node\n",
        "\n",
        "    def set_predicat(self, predicat):\n",
        "        self.predicat = predicat\n",
        "\n",
        "    def set_class(self, c):\n",
        "        self.c = c\n",
        "        \n",
        "    def set_positives(self, positives):\n",
        "        self.positives = positives\n",
        "        \n",
        "    def set_negatives(self, negatives):\n",
        "        self.negatives = negatives\n",
        "\n",
        "    #getters:\n",
        "    def get_left(self):\n",
        "        return self.left\n",
        "\n",
        "    def get_right(self):\n",
        "        return self.right\n",
        "\n",
        "    def get_class(self):\n",
        "        return self.c\n",
        "\n",
        "    def get_idxs(self):\n",
        "        return self.idxs\n",
        "    \n",
        "    def get_positives(self):\n",
        "        return self.positives\n",
        "        \n",
        "    def get_negatives(self):\n",
        "        return self.negatives\n",
        "    \n",
        "    def get_len(self):\n",
        "        return self.idxs.shape[0]\n",
        "\n",
        "    #checkers:\n",
        "    def is_leaf(self):\n",
        "        return self.predicat is None\n",
        "\n",
        "    def is_inner(self):\n",
        "        return not self.is_leaf()\n",
        "    \n",
        "    def make_leaf(self):\n",
        "        self.predicat = None\n",
        "        self.left = None\n",
        "        self.right = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ9hwxkgdLt-",
        "colab_type": "text"
      },
      "source": [
        "#### Решающее дерево"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1_Hy4KzdLt_",
        "colab_type": "text"
      },
      "source": [
        "Критерии информативности:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1lo0P5RdLt_",
        "colab_type": "text"
      },
      "source": [
        "Это новая реализация, которая работает за константное время, если в качестве аргументов предать количество положительных и отрицательных элементов в наборе вместо самого набора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFUkIlMQdLuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bingini(*args):\n",
        "    # for binary classification: p(-) = 1 - p(+)=>\n",
        "    # Gini = 2 * p(+) * (1 - p(+))\n",
        "    if len(args) == 2:\n",
        "        p = args[0] / (args[1]+args[0])\n",
        "    else:\n",
        "        p = args[0].sum() / args[0].shape[0]\n",
        "    return 2 * p * (1 - p)\n",
        "\n",
        "def binentropy(*args):\n",
        "    # for binary classification: p(-) = 1 - p(+)=>\n",
        "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
        "    if len(args) == 2:\n",
        "        p = args[0] / (args[1]+args[0])\n",
        "    else:\n",
        "        p = args[0].sum() / args[0].shape[0]\n",
        "    return -p*np.log(p) - (1 - p)*np.log(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7DuJ0iWdLuC",
        "colab_type": "text"
      },
      "source": [
        "Реализация решающего дерева с возможностью наращивания дерева с помомщью метода случайных подпространств, что нужно для использования класса в качестве базового при строительстве *Случайного Леса*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az8yARHydLuC",
        "colab_type": "text"
      },
      "source": [
        "<span style=\"color:springgreen\">Внимание:</span> Здесь представлена ускоренная реализация дерева по отношению к старой реализации, котроую можно посмотреть ниже. Здесь ускоряется поиск разделения вершины по порогу и признаку дерева на 2 дочерних вершины. Это делается путем:\n",
        "- Выделения условно категориальных признаков, что позволяет искать разделение просто перебрав возможные значения.\n",
        "- Для условно непрерывных признаков производится предваврительная сортировка, что дает возможность вычисление критерия информативности и критерия качества за константное время во время прохода по значениям столбца."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joHQhJDedLuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryDescisionTree:\n",
        "    def __init__(self, criteria=bingini, pruning_cost=None, min_samples_split=2, random_sub_num=None):\n",
        "        self.CATEGORICAL_LEN = 10\n",
        "        self.is_categorical = None\n",
        "        self.categorical_vals = {}\n",
        "        self.H = criteria\n",
        "        self.root = None\n",
        "        self.min_split = min_samples_split\n",
        "        if random_sub_num is None:\n",
        "            self.random_subspace = False\n",
        "        else:\n",
        "            self.random_subspace = True\n",
        "            self.feature_num = random_sub_num\n",
        "        if pruning_cost is None:\n",
        "            self.pruning = False\n",
        "        else:\n",
        "            self.pruning = True\n",
        "            self.alpha = pruning_cost\n",
        "            \n",
        "    def node_classify(self, node):\n",
        "        # set class num node:\n",
        "        positives = node.get_positives()\n",
        "        negatives = node.get_negatives()\n",
        "        if positives >= negatives:\n",
        "            node.set_class(1)\n",
        "        else:\n",
        "            node.set_class(0)\n",
        "            \n",
        "    # create root and recursive building tree\n",
        "    def build_tree(self):\n",
        "        # create root with all nums\n",
        "        self.root = BinaryNode(np.arange(self.Y.shape[0]))\n",
        "        self.root.set_positives(self.Y.sum())\n",
        "        negatives = self.root.get_len() - self.root.get_positives()\n",
        "        self.root.set_negatives(negatives)\n",
        "        # recursive function of creation\n",
        "        self.recursive_creation(self.root)\n",
        "            \n",
        "            \n",
        "    # may be modifed\n",
        "    def stop_criteria(self, node):\n",
        "        # if num of eelems in node less than min required for split => 1\n",
        "        if node.get_len() < self.min_split:\n",
        "            return True\n",
        "        # if all elems in node has only one class => 1\n",
        "        positives = node.get_positives()\n",
        "        negatives = node.get_negatives()\n",
        "        return (negatives==0 or positives==0)\n",
        "    \n",
        "    # union of search_best_split() and split_node()\n",
        "    def search_best_split(self, node):\n",
        "        X_iter = self.X[node.get_idxs()]\n",
        "        Y_iter = self.Y[node.get_idxs()]\n",
        "        # compute node info criteria\n",
        "        positiv = node.get_positives()\n",
        "        negativ = node.get_negatives()\n",
        "        node_info = self.H(positiv, negativ)\n",
        "        # best params\n",
        "        best_gain = 0.0\n",
        "        best_j, best_t = 0, 0.0\n",
        "        # search in all features:\n",
        "        if self.random_subspace:\n",
        "            # get random permutation\n",
        "            features = np.random.permutation(self.X.shape[1])\n",
        "            # stay only feature_num random features\n",
        "            features = features[:self.feature_num]\n",
        "        # else search by all features\n",
        "        else:\n",
        "            features = range(self.X.shape[1])\n",
        "\n",
        "        for j in features:\n",
        "            column = X_iter[:, j]\n",
        "            # fast search if categorical:\n",
        "            if self.is_categorical[j]:\n",
        "                possible_vals = self.categorical_vals[j]\n",
        "                for i in range(1, possible_vals.shape[0]):\n",
        "                    mask = column < possible_vals[i]\n",
        "                    Y_r = Y_iter[mask]\n",
        "                    if Y_r.shape[0] == 0 or Y_r.shape[0] == Y_iter.shape[0]:\n",
        "                        continue\n",
        "                    right_pos = Y_r.sum()\n",
        "                    right_neg = Y_r.shape[0] - right_pos\n",
        "                    right_gini = self.H(right_pos, right_neg)\n",
        "                    left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
        "                    # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
        "                    gain = node_info\n",
        "                    gain -= (Y_r.shape[0]*right_gini/node.get_len())\n",
        "                    gain -= (1 - Y_r.shape[0]/node.get_len())*left_gini\n",
        "                    if gain > best_gain:\n",
        "                        best_t = possible_vals[i]\n",
        "                        best_j = j\n",
        "                        best_gain = gain  \n",
        "                continue\n",
        "            # else standart search:\n",
        "            sorted_col = np.argsort(column)\n",
        "            right_neg = 0\n",
        "            right_pos = 0\n",
        "            last_t = column[sorted_col[0]]\n",
        "            for i in range(1, column.shape[0]):\n",
        "                if Y_iter[sorted_col[i-1]]:\n",
        "                    right_pos += 1\n",
        "                else:\n",
        "                    right_neg += 1\n",
        "                    \n",
        "                idx = sorted_col[i]\n",
        "                if column[idx] == last_t:\n",
        "                    continue\n",
        "                \n",
        "                last_t = column[idx]\n",
        "                # compute gain:\n",
        "                right_gini = self.H(right_pos, right_neg)\n",
        "                left_gini = self.H(positiv - right_pos, negativ - right_neg)\n",
        "                # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
        "                gain = node_info\n",
        "                gain -= (i*right_gini/node.get_len()) + (1 - i/node.get_len())*left_gini\n",
        "                # needs best gain split\n",
        "                if gain > best_gain:\n",
        "                    best_t = column[idx]\n",
        "                    best_j = j\n",
        "                    best_gain = gain\n",
        "                    \n",
        "        if best_gain > 0.0:\n",
        "            return best_j, best_t\n",
        "    \n",
        "    # create 2 new nodes: left and right\n",
        "    def split_node(self, node, j, t):\n",
        "        # set predicat rule for node\n",
        "        predicat = lambda x: x[j] < t\n",
        "        node.set_predicat(predicat)\n",
        "        # get split mask\n",
        "        column = self.X[node.get_idxs(), j]\n",
        "        mask = column < t\n",
        "        # make idxs for left and right\n",
        "        right_idxs = node.get_idxs()[mask]\n",
        "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
        "        #compute positives:\n",
        "        right_pos = self.Y[right_idxs].sum()\n",
        "        left_pos = self.Y[left_idxs].sum()\n",
        "        # compute negatives\n",
        "        right_neg = right_idxs.shape[0] - right_pos\n",
        "        left_neg = left_idxs.shape[0] - left_pos\n",
        "        # create nodes\n",
        "        node.set_left(BinaryNode(left_idxs, left_pos, left_neg))\n",
        "        node.set_right(BinaryNode(right_idxs, right_pos, right_neg))\n",
        "    \n",
        "    # recursive function for nodes:\n",
        "    def recursive_creation(self, node):\n",
        "        # classify another node:\n",
        "        self.node_classify(node)\n",
        "        # stop criteria for building\n",
        "        if self.stop_criteria(node):\n",
        "            return\n",
        "        #else find best split\n",
        "        jt = self.search_best_split(node)\n",
        "        # if we cant find best split - stop\n",
        "        if jt is None:\n",
        "            return\n",
        "\n",
        "        # split node for 2 child:\n",
        "        self.split_node(node, *jt)\n",
        "        # start recursion for left:\n",
        "        self.recursive_creation(node.get_left())\n",
        "        # for right:\n",
        "        self.recursive_creation(node.get_right())\n",
        "    \n",
        "    def tree_pruning(self, node):\n",
        "        # this node R_a(t) computing:\n",
        "        # R = sum([y != c]) / |N|\n",
        "        if node.get_class():\n",
        "            R_a = node.get_negatives() / node.get_len()\n",
        "        else:\n",
        "            R_a = node.get_positives() / node.get_len()\n",
        "        # R_a(t) = R(t) + a\n",
        "        R_a += self.alpha\n",
        "        # at first go while not leaf:\n",
        "        if node.is_leaf():\n",
        "            # return R_a(leaf)\n",
        "            return R_a\n",
        "        # R_a(Tl) and R_a(Tr)\n",
        "        R_al = self.tree_pruning(node.get_left())\n",
        "        R_ar = self.tree_pruning(node.get_right())\n",
        "        # if R_a(t) < R_a(T) => pruning\n",
        "        if R_a <= R_al + R_ar:\n",
        "            node.make_leaf()\n",
        "            return R_a\n",
        "        # else do nothing\n",
        "        return R_al + R_ar\n",
        "    \n",
        "    def search_categorical(self):\n",
        "        self.is_categorical = np.zeros(self.X.shape[1]).astype(np.int8)\n",
        "        for j in range(self.X.shape[1]):\n",
        "            uniq = np.unique(self.X[:, j])\n",
        "            if uniq.shape[0] < self.CATEGORICAL_LEN:\n",
        "                self.categorical_vals[j] = uniq\n",
        "                self.is_categorical[j] = 1\n",
        "            \n",
        "\n",
        "    def fit(self, X_train, Y_train):\n",
        "        # temp sets for comfort\n",
        "        self.X = X_train\n",
        "        self.Y = Y_train\n",
        "        # we will store only idxs while training in nodes =>\n",
        "        # make idxs column for comfort:\n",
        "        self.idxs = np.arange(X_train.shape[0])\n",
        "        # for speed search categorical\n",
        "        self.search_categorical()\n",
        "        # build tree \n",
        "        self.build_tree()\n",
        "        # pruning tree\n",
        "        if self.pruning:\n",
        "            self.tree_pruning(self.root)\n",
        "        #  delete temp sets:\n",
        "        #self.free_memory(root)\n",
        "        del self.X\n",
        "        del self.Y\n",
        "        del self.idxs\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
        "        # for each elem in X predict result:\n",
        "        for i in np.arange(X_val.shape[0]):\n",
        "            Y_pred[i] = self.predict_one(X_val[i])\n",
        "        return Y_pred\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        node = self.root\n",
        "        while node.is_inner():\n",
        "            if node.predicat(x):\n",
        "                node = node.get_right()\n",
        "            else:\n",
        "                node = node.get_left()\n",
        "        # if in leaf:\n",
        "        return node.get_class()\n",
        "\n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Decision Tree\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Tree\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZDFZCdIdLuF",
        "colab_type": "text"
      },
      "source": [
        "Возможно стоит в будущем добавить визуализацию дерева, поскольку интерпретируемость - важное свойство дерева."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QznsiZGBdLuF",
        "colab_type": "text"
      },
      "source": [
        "#### Старая реализаци"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtj-FRHVdLuG",
        "colab_type": "text"
      },
      "source": [
        "<span style=\"color:red\">Внимание:</span> Здесь показана моя старая реализация дерева, которая сильно медленне модификации, представленной выше. Её можно запустить и скорей всего получить корректный ответ, но на больших данных она показывает чудовищно большое время работы.\n",
        "\n",
        "Её можно удалить от сюда, для сокращения количества содержимого. Далее она не используется."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBA-yoEudLuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OldBinaryNode:\n",
        "    def __init__(self, idxs = None, c = None):\n",
        "        self.predicat = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.positives = None\n",
        "        self.negatives = None\n",
        "        if c is not None:\n",
        "            self.c = c\n",
        "        if idxs is not None:\n",
        "            self.idxs = idxs\n",
        "\n",
        "    # setters:\n",
        "    def set_left(self, left_node):\n",
        "        self.left = left_node\n",
        "\n",
        "    def set_idxs(self, idxs):\n",
        "        self.idxs = idxs\n",
        "\n",
        "    def set_right(self, right_node):\n",
        "        self.right = right_node\n",
        "\n",
        "    def set_predicat(self, predicat):\n",
        "        self.predicat = predicat\n",
        "\n",
        "    def set_class(self, c):\n",
        "        self.c = c\n",
        "\n",
        "    #getters:\n",
        "    def get_left(self):\n",
        "        return self.left\n",
        "\n",
        "    def get_right(self):\n",
        "        return self.right\n",
        "\n",
        "    def get_class(self):\n",
        "        return self.c\n",
        "\n",
        "    def get_idxs(self):\n",
        "        return self.idxs\n",
        "\n",
        "    #checkers:\n",
        "    def is_leaf(self):\n",
        "        return self.predicat is None\n",
        "\n",
        "    def is_inner(self):\n",
        "        return not self.is_leaf()\n",
        "    \n",
        "    def make_leaf(self):\n",
        "        self.predicat = None\n",
        "        self.left = None\n",
        "        self.right = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDmr7tEudLuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oldbingini(Y):\n",
        "    # for binary classification: p(-) = 1 - p(+)=>\n",
        "    # Gini = 2 * p(+) * (1 - p(+))\n",
        "    p = Y.sum() / len(Y)\n",
        "    return 2 * p * (1 - p)\n",
        "\n",
        "def oldbinentropy(Y):\n",
        "    # for binary classification: p(-) = 1 - p(+)=>\n",
        "    # Entropy = -p(+)log(p(+)) - (1 - p(+))log(1 - p(+))\n",
        "    p = Y.sum() / len(Y)\n",
        "    return -p*np.log(p) - (1 - p)*np.log(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYMsJ-hYdLuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if without pruning => pruning_cost = None\n",
        "# if without random subspace building => rndm_sub_num - None\n",
        "class OldBinaryDescisionTree:\n",
        "    def __init__(self, criteria=oldbingini, pruning_cost=None, min_samples_split=2):\n",
        "        self.H = criteria\n",
        "        self.root = None\n",
        "        self.min_split = min_samples_split\n",
        "        if pruning_cost is None:\n",
        "            self.pruning = False\n",
        "        else:\n",
        "            self.pruning = True\n",
        "            self.alpha = pruning_cost\n",
        "\n",
        "\n",
        "    # functional Q for separation\n",
        "    def functional(self, idxs, j, t):\n",
        "        # Q(Rm, j, t) = H(Rm) - (|Rl|/|Rm|)H(Rl) - (|Rr|/|Rm|)H(Rr)\n",
        "        RmX = self.X[idxs]\n",
        "        RmY = self.Y[idxs]\n",
        "        # make split:\n",
        "        mask = RmX[:, j] < t\n",
        "        RrY = RmY[mask]\n",
        "        RlY = RmY[np.logical_not(mask)]\n",
        "\n",
        "        return self.H(RmY) - len(RrY)*self.H(RrY) / len(RmY) - len(RlY)*self.H(RlY) / len(RmY)\n",
        "        \n",
        "    # may be modifed\n",
        "    def stop_criteria(self, node):\n",
        "        Y = self.Y[node.get_idxs()]\n",
        "        # if num of eelems in node less than min required for split => 1\n",
        "        if len(Y) < self.min_split:\n",
        "            return True\n",
        "        # if all elems in node has only one class => 1\n",
        "        positives = Y.sum()\n",
        "        return positives == len(Y) or positives == 0\n",
        "\n",
        "\n",
        "    # create root and recursive building tree\n",
        "    def build_tree(self):\n",
        "        # create root with all nums\n",
        "        self.root = OldBinaryNode(np.arange(len(self.Y)))\n",
        "        self.recursive_creation(self.root)\n",
        "\n",
        "    # compute gains and get best varios:\n",
        "    def search_best_split(self, node):\n",
        "        idxs = node.get_idxs()\n",
        "        best_gain = 0.0\n",
        "        best_j, best_t = 0, 0.0\n",
        "        # itterate for all possible values\n",
        "        X_iter = self.X[idxs]\n",
        "        for j in range(self.X.shape[1]):\n",
        "            for t in X_iter[:, j]:\n",
        "                gain = self.functional(idxs, j, t)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_j = j\n",
        "                    best_t = t\n",
        "        if best_gain > 0.0:\n",
        "            return best_j, best_t\n",
        "\n",
        "    # create 2 new nodes: left and right\n",
        "    def split_node(self, node, j, t):\n",
        "        # set predicat rule for node\n",
        "        predicat = lambda x: x[j] < t\n",
        "        node.set_predicat(predicat)\n",
        "        # get split mask\n",
        "        column = self.X[node.get_idxs(), j]\n",
        "        mask = column < t\n",
        "        # make idxs for left and right\n",
        "        right_idxs = node.get_idxs()[mask]\n",
        "        left_idxs = node.get_idxs()[np.logical_not(mask)]\n",
        "        # create nodes\n",
        "        node.set_left(OldBinaryNode(left_idxs))\n",
        "        node.set_right(OldBinaryNode(right_idxs))\n",
        "\n",
        "\n",
        "    def node_classify(self, node):\n",
        "        # set class num node:\n",
        "        positives = self.Y[node.get_idxs()].sum()\n",
        "        if positives >= (len(node.get_idxs()) + 1) // 2:\n",
        "            node.set_class(1)\n",
        "        else:\n",
        "            node.set_class(0)\n",
        "\n",
        "\n",
        "    # recursive function for nodes:\n",
        "    def recursive_creation(self, node):\n",
        "        # classify another node:\n",
        "        self.node_classify(node)\n",
        "        # stop criteria for building\n",
        "        if self.stop_criteria(node):\n",
        "            return\n",
        "        #else find best split\n",
        "        jt = self.search_best_split(node)\n",
        "        # if we cant find best split - stop\n",
        "        if jt is None:\n",
        "            return\n",
        "        # split node for 2 child:\n",
        "        self.split_node(node, *jt)\n",
        "        # start recursion for left:\n",
        "        self.recursive_creation(node.get_left())\n",
        "        # for right:\n",
        "        self.recursive_creation(node.get_right())\n",
        "\n",
        "    def tree_pruning(self, node):\n",
        "        # this node R_a(t) computing:\n",
        "        # R = sum([y != c]) / |N|\n",
        "        R_a = (self.Y[node.get_idxs()] != node.get_class()).sum() / node.get_idxs().shape[0]\n",
        "        # R_a(t) = R(t) + a\n",
        "        R_a += self.alpha\n",
        "        # at first go while not leaf:\n",
        "        if node.is_leaf():\n",
        "            # return R_a(leaf)\n",
        "            return R_a\n",
        "        # R_a(Tl) and R_a(Tr)\n",
        "        R_al = self.tree_pruning(node.get_left())\n",
        "        R_ar = self.tree_pruning(node.get_right())\n",
        "        # if R_a(t) < R_a(T) => pruning\n",
        "        if R_a <= R_al + R_ar:\n",
        "            node.make_leaf()\n",
        "            return R_a\n",
        "        # else do nothing\n",
        "        return R_al + R_ar\n",
        "        \n",
        "\n",
        "    def fit(self, X_train, Y_train):\n",
        "        # temp sets for comfort\n",
        "        self.X = X_train\n",
        "        self.Y = Y_train\n",
        "        # we will store only idxs while training in nodes =>\n",
        "        # make idxs column for comfort:\n",
        "        self.idxs = np.arange(X_train.shape[0])\n",
        "        # build tree \n",
        "        self.build_tree()\n",
        "        # pruning tree\n",
        "        if self.pruning:\n",
        "            self.tree_pruning(self.root)\n",
        "        #  delete temp sets:\n",
        "        #self.free_memory(root)\n",
        "        del self.X\n",
        "        del self.Y\n",
        "        del self.idxs\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        Y_pred = np.zeros(X_val.shape[0]).astype(np.int8)\n",
        "        # for each elem in X predict result:\n",
        "        for i in np.arange(X_val.shape[0]):\n",
        "            Y_pred[i] = self.predict_one(X_val[i])\n",
        "        return Y_pred\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        node = self.root\n",
        "        while node.is_inner():\n",
        "            if node.predicat(x):\n",
        "                node = node.get_right()\n",
        "            else:\n",
        "                node = node.get_left()\n",
        "        # if in leaf:\n",
        "        return node.get_class()\n",
        "\n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Decision Tree\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Tree\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BmpUeuUdLuN",
        "colab_type": "text"
      },
      "source": [
        "Можно удалить это реализацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bnBwTEjdLuN",
        "colab_type": "text"
      },
      "source": [
        "### Метод k ближайших соседей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE0Ogy-xdLuO",
        "colab_type": "text"
      },
      "source": [
        "#### Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s29HpLD_dLuO",
        "colab_type": "text"
      },
      "source": [
        "*Метод k ближайших соседей* классифицирует объекты лениво, то есть не требует настройки параметров по обучающей выборке, а просто запоминает её и делает предсказания на основании классов ближайших соседей предсказываемого объекта. Для измерения близости необходимо задать метрику $\\rho(x, y)$.\n",
        "\n",
        "Для предсказания класса $c$ объекта $x$ необходимо найти в обучающей выборке такие $k$ объектов c наименьшим растоянием до объекта $\\Omega = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^{k+1}, \\; \\Omega \\subset (X, Y): \\; \\; \\rho(x, x^{(0)}) \\leq ... \\leq \\rho(x, x^{(i)}) \\leq ... \\leq \\rho(x, x^{(k)})$ и $\\rho(x, x_1) \\leq \\rho(x, x_2), \\; \\forall (x_1, y_1) \\in \\Omega, \\; \\forall (x_2, y_2) \\in (X, Y) \\setminus \\Omega$. Тогда класс объекта согласно *Методу окна Парзена переменной ширины* определяется по формуле:\n",
        "\n",
        "$$c = \\underset{y \\in Y}{\\operatorname{argmax}} \\sum\\limits^{k}_{i = 1} [y = y^{(i)}] K \\left( \\displaystyle\\frac{\\rho(x, x^{i})}{\\rho(x, x^{k+1})} \\right)$$\n",
        "\n",
        "Здесь $K(t)$ - функция(ядро), неубывающая на $t \\in [0, 1]$. При $K(t)  = 1 = const$ *Метод окна Парзена* становится *Методом k ближайших соседей.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5VGuzNedLuO",
        "colab_type": "text"
      },
      "source": [
        "#### k-nearest neighbors algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-l5CnTqdLuP",
        "colab_type": "text"
      },
      "source": [
        "Возможные метрики для алгоритма:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLxmvZnldLuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minkovski(x1, x2, p = 3):\n",
        "    return (np.abs(x1 - x2) ** p).T.sum(axis = 0)**(1.0 / p)\n",
        "\n",
        "def euclid(x1, x2):\n",
        "    return minkovski(x1, x2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr7721X0dLuS",
        "colab_type": "text"
      },
      "source": [
        "Возможные невозрастающие ядра $K(t) \\in [0, 1], \\; t \\in [0, 1]$ для обобщения метода до *Метода окна Парзена:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wjZIxWSdLuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def biquadratical_kernel(t):\n",
        "    return (-t**2 + 1)**2\n",
        "\n",
        "def triquadratical_kernel(t):\n",
        "    return (-t**2 + 1)**3\n",
        "    \n",
        "def triqubical_kernel(t):\n",
        "    return (-t**3 + 1)**3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsHd8YmOdLuV",
        "colab_type": "text"
      },
      "source": [
        "Реализация метода с возможностью учета расстояний между соседями, однако без отбора эталонных элементов и эвристик, ускоряющих поиск соседей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfI6wNwEdLuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stay parsen kerel None for simple KNN algo\n",
        "class BinaryKNN:\n",
        "    def __init__(self, k, metric = euclid, parsen_kernel = None):\n",
        "        self.k = k\n",
        "        self.p = metric\n",
        "        # algo shoud remember all data\n",
        "        self.X = None\n",
        "        self.Y = None\n",
        "        if parsen_kernel is None:\n",
        "            # all elems have equal weight\n",
        "            self.Kp = lambda t: 1.0\n",
        "        else:\n",
        "            # parsen method\n",
        "            self.Kp = parsen_kernel\n",
        "\n",
        "    def stolp_filtration(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X_train, Y_train):\n",
        "        # just remmember data:\n",
        "        self.X = X_train\n",
        "        self.Y = Y_train\n",
        "        # check correct of k\n",
        "        self.k = min(self.k, len(self.Y) - 1)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_val):\n",
        "        Y_pred = np.zeros(len(X_val)).astype(np.int8)\n",
        "        # for each elem in X predict result:\n",
        "        for i in np.arange(len(X_val)):\n",
        "            Y_pred[i] = self.predict_one(X_val[i])\n",
        "        return Y_pred\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        # compute all distances\n",
        "        r_x = self.p(self.X, x)\n",
        "        # take sorted order of dists in idxs\n",
        "        order = np.argsort(r_x)\n",
        "        # width for parsen is distance to k+1 neiborh:\n",
        "        h = r_x[order[self.k]]\n",
        "        # idxs of first k elems neibrh\n",
        "        order = order[:self.k]\n",
        "        # take first k Y:\n",
        "        Y_k = self.Y[order]\n",
        "        # compute parsen function for all neiborh:\n",
        "        K = self.Kp(r_x[order] / h)\n",
        "        \n",
        "        # compute functional for positives elems\n",
        "        pos_w = (K * Y_k).sum()\n",
        "        # compute functional for negatives elems\n",
        "        neg_w = (K * np.logical_not(Y_k)).sum()\n",
        "        \n",
        "        # class with more functional wins\n",
        "        return int(pos_w > neg_w) # 0 or 1\n",
        "\n",
        "\n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"k Nearest Neighbor\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"KNN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA31EebDdLuY",
        "colab_type": "text"
      },
      "source": [
        "В будущем можно будет добавить реализацию алгоритма **STOLP**, для классификации только по эталонным объектам. Было решено не добавлять, так как появится 1-2 новых параметра алгоритма, которые также будет необходимо подбирать, что усложнит работу с классом для людей незнакомыми со **STOLP**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeZsCrgJdLuY",
        "colab_type": "text"
      },
      "source": [
        "### Случайный лес"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLi-qbadLuZ",
        "colab_type": "text"
      },
      "source": [
        "#### Описание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mv8u5CfdLuZ",
        "colab_type": "text"
      },
      "source": [
        "Метод построения композиции классификаторов, называемый *Случайный Лес*, призван бороться с главным недостатком решающих деревьев - переобучением.\n",
        "\n",
        "Если дана обучающая выборка $(X, Y)$, алгоритм построения леса из $N$ деревьев выглядит так:\n",
        "\n",
        "Для каждого $k = 1 \\dots N$:\n",
        "\n",
        "1. Генерируем случайную подвыборку с повторениями $(X_k, Y_k)$\n",
        "2. Начинаем строить решающее дерево $b_k$ на выборке $(X_k, Y_k)$:\n",
        "   - На каждом сплите выбираем случайное подмножество признаков $d_l$\n",
        "   - Из этого подмножества $d_l$ берем тот признак, сплит по которому является наилучшим согласно критерию\n",
        "   - Строим дерево до тех пор, пока не достигается максимально указанная глубина _max_depth_ или в листьях не оказывается _n_min_ объектов\n",
        "   \n",
        "Получаем итоговый ансабмль моделей (лес деревьев) в виде голосования моделей:\n",
        "\n",
        "$$\\large a(x) = \\frac{1}{N}\\sum_{k = 1}^N b_k(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_D6GzJidLuZ",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe7f1FAKdLua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryRandomForest:\n",
        "    def __init__(self, Ntrees=20, criteria=bingini, random_sub_num=None):\n",
        "        self.N = Ntrees\n",
        "        self.criteria = criteria\n",
        "        self.sub_space_num = random_sub_num\n",
        "        # list for trained models\n",
        "        self.trees = [None for _ in range(self.N)]\n",
        "\n",
        "    # get random sample for tree training\n",
        "    def bootstrap_sample(self, X, Y):\n",
        "        # indexes of X and Y\n",
        "        indexes = np.arange(len(Y))\n",
        "        # rundom indexes with repeats\n",
        "        indexes = np.random.choice(indexes, len(indexes))\n",
        "        # bootstrap sample\n",
        "        return X[indexes], Y[indexes]\n",
        "    \n",
        "    def fit(self, X_train, Y_train):\n",
        "        if self.sub_space_num is None:\n",
        "            self.sub_space_num = int(len(X_train)**(1/2))\n",
        "        # train N trees with \n",
        "        for i in range(self.N):\n",
        "            # create tree(use our class of tree)\n",
        "            self.trees[i] = BinaryDescisionTree(\n",
        "                self.criteria, # user criteria\n",
        "                None, # trees without prunning\n",
        "                2, # build while it possible \n",
        "                self.sub_space_num) # num of random features in random space method\n",
        "            # and train tree:\n",
        "            self.trees[i].fit(*self.bootstrap_sample(X_train, Y_train))\n",
        "        return self\n",
        "            \n",
        "\n",
        "    def predict(self, X_val):\n",
        "        voices = np.zeros(len(X_val))\n",
        "        # make vote:\n",
        "        for tree in self.trees:\n",
        "            voices += tree.predict(X_val)\n",
        "        # compute winners:\n",
        "        Y_pred = voices >= (self.N + 1) // 2\n",
        "        return Y_pred.astype(np.int8)\n",
        "\n",
        "    \n",
        "    def score(self, X_val, Y_val, metric=Accuracy):\n",
        "        return metric(Y_val, self.predict(X_val))\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Random Forest\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"RF\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6HHGxGdLud",
        "colab_type": "text"
      },
      "source": [
        "Далее не представлена демонстрация работы *Случайного леса*, поскольку у меня не было его по заданию, а ждать его обучения вместе с кросс-валидацией слишком долго. Однако его работоспособность обробованна на других датасетах. Можете попробовать и вы!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ded9QLxudLwB",
        "colab_type": "code",
        "colab": {},
        "outputId": "6f8244f1-a87a-4976-a04e-1ddcaf0a6a26"
      },
      "source": [
        "my_svm_model.score(X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8421663892222523"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}